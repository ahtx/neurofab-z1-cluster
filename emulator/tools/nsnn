#!/usr/bin/env python3
"""
nsnn - Manage Spiking Neural Networks on Z1 cluster

Comprehensive SNN management utility with multi-backplane support for 200+ nodes.
"""

import sys
import os
import argparse
import json
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# Add lib directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lib'))

from z1_client import Z1Client, Z1ClusterError
from cluster_config import ClusterConfig
from snn_compiler import compile_snn_topology, DeploymentPlan


def deploy_snn(args):
    """Deploy SNN topology to cluster."""
    print(f"Loading topology: {args.topology}")
    
    # Load cluster configuration if multi-backplane
    config = None
    if args.config or args.all:
        config = ClusterConfig(args.config)
        if len(config) == 0:
            print("Error: No backplanes configured", file=sys.stderr)
            return 1
        
        # Convert to backplane_config format for compiler
        backplane_config = {
            'backplanes': [
                {
                    'name': bp.name,
                    'controller_ip': bp.controller_ip,
                    'node_count': bp.node_count
                }
                for bp in config.backplanes
            ]
        }
    else:
        backplane_config = None
    
    # Compile topology
    print("Compiling SNN topology...")
    deployment_plan = compile_snn_topology(args.topology, backplane_config)
    
    print(f"\nDeployment Plan:")
    print(f"  Total Neurons:  {deployment_plan.total_neurons}")
    print(f"  Total Synapses: {deployment_plan.total_synapses}")
    print(f"  Backplanes:     {len(deployment_plan.backplane_nodes)}")
    print(f"  Nodes:          {len(deployment_plan.neuron_tables)}")
    
    # Group by backplane
    print(f"\nDistribution:")
    for bp_name, node_list in deployment_plan.backplane_nodes.items():
        total_neurons_bp = sum(
            len(deployment_plan.neuron_tables[(bp_name, nid)]) // 256
            for nid in node_list
        )
        print(f"  {bp_name}: {len(node_list)} nodes, {total_neurons_bp} neurons")
    
    # Deploy to each backplane
    print(f"\nDeploying neuron tables...")
    
    deployment_results = {}
    
    for bp_name, node_list in deployment_plan.backplane_nodes.items():
        # Get controller IP for this backplane
        if config:
            bp = config.get_backplane(bp_name)
            if not bp:
                print(f"Error: Backplane '{bp_name}' not found in configuration", file=sys.stderr)
                continue
            controller_ip = bp.controller_ip
        else:
            controller_ip = args.controller
        
        print(f"\n  Deploying to {bp_name} ({controller_ip})...")
        client = Z1Client(controller_ip=controller_ip)
        
        # Deploy to each node on this backplane
        for node_id in node_list:
            table_data = deployment_plan.neuron_tables[(bp_name, node_id)]
            neuron_count = len(table_data) // 256
            
            try:
                # Write neuron table to node memory
                # Address: 0x20100000 (1MB offset in PSRAM)
                addr = 0x20100000
                bytes_written = client.write_memory(node_id, addr, table_data)
                
                print(f"    Node {node_id:2d}: {neuron_count:4d} neurons ({bytes_written:6d} bytes) ✓")
                
                deployment_results[(bp_name, node_id)] = {
                    'success': True,
                    'neurons': neuron_count,
                    'bytes': bytes_written
                }
                
            except Exception as e:
                print(f"    Node {node_id:2d}: ERROR - {e}")
                deployment_results[(bp_name, node_id)] = {
                    'success': False,
                    'error': str(e)
                }
    
    # Summary
    successful = sum(1 for r in deployment_results.values() if r.get('success'))
    total = len(deployment_results)
    
    print(f"\nDeployment Summary:")
    print(f"  Successful: {successful}/{total} nodes")
    print(f"  Total neurons deployed: {deployment_plan.total_neurons}")
    
    if successful == total:
        print(f"\n✓ SNN deployed successfully!")
        
        # Save deployment info for later use
        deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
        os.makedirs(os.path.dirname(deployment_info_file), exist_ok=True)
        
        with open(deployment_info_file, 'w') as f:
            json.dump({
                'topology_file': args.topology,
                'deployment_plan': {
                    'total_neurons': deployment_plan.total_neurons,
                    'total_synapses': deployment_plan.total_synapses,
                    'backplane_nodes': deployment_plan.backplane_nodes,
                    'neuron_map': {
                        str(k): v for k, v in deployment_plan.neuron_map.items()
                    }
                },
                'timestamp': time.time()
            }, f, indent=2)
        
        return 0
    else:
        print(f"\n✗ Deployment failed on {total - successful} nodes", file=sys.stderr)
        return 1


def status_snn(args):
    """Show SNN status."""
    # Load last deployment info
    deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
    
    if not os.path.exists(deployment_info_file):
        print("No SNN deployed")
        return 0
    
    with open(deployment_info_file, 'r') as f:
        info = json.load(f)
    
    deployment_plan = info['deployment_plan']
    
    print("SNN Status:")
    print("=" * 60)
    print(f"  Topology:       {os.path.basename(info['topology_file'])}")
    print(f"  Total Neurons:  {deployment_plan['total_neurons']}")
    print(f"  Total Synapses: {deployment_plan['total_synapses']}")
    print(f"  Backplanes:     {len(deployment_plan['backplane_nodes'])}")
    
    print(f"\nDistribution:")
    for bp_name, node_list in deployment_plan['backplane_nodes'].items():
        print(f"  {bp_name}: {len(node_list)} nodes")
    
    return 0


def start_snn(args):
    """Start SNN execution."""
    print("Starting SNN execution...")
    
    # Load deployment info
    deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
    if not os.path.exists(deployment_info_file):
        print("Error: No SNN deployed. Use 'nsnn deploy' first.", file=sys.stderr)
        return 1
    
    with open(deployment_info_file, 'r') as f:
        info = json.load(f)
    
    deployment_plan = info['deployment_plan']
    
    # Get cluster configuration
    config = ClusterConfig(args.config)
    
    # Start SNN on each backplane
    for bp_name in deployment_plan['backplane_nodes'].keys():
        bp = config.get_backplane(bp_name)
        if not bp:
            print(f"Warning: Backplane '{bp_name}' not found in configuration", file=sys.stderr)
            continue
        
        try:
            client = Z1Client(controller_ip=bp.controller_ip)
            client.start_snn()
            print(f"  {bp_name}: Started ✓")
        except Exception as e:
            print(f"  {bp_name}: ERROR - {e}", file=sys.stderr)
    
    print("\n✓ SNN started")
    return 0


def stop_snn(args):
    """Stop SNN execution."""
    print("Stopping SNN execution...")
    
    # Load deployment info
    deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
    if not os.path.exists(deployment_info_file):
        print("Warning: No deployment info found")
        return 0
    
    with open(deployment_info_file, 'r') as f:
        info = json.load(f)
    
    deployment_plan = info['deployment_plan']
    
    # Get cluster configuration
    config = ClusterConfig(args.config)
    
    # Stop SNN on each backplane
    for bp_name in deployment_plan['backplane_nodes'].keys():
        bp = config.get_backplane(bp_name)
        if not bp:
            continue
        
        try:
            client = Z1Client(controller_ip=bp.controller_ip)
            client.stop_snn()
            print(f"  {bp_name}: Stopped ✓")
        except Exception as e:
            print(f"  {bp_name}: ERROR - {e}", file=sys.stderr)
    
    print("\n✓ SNN stopped")
    return 0


def monitor_snn(args):
    """Monitor SNN spike activity."""
    duration_ms = args.duration
    
    print(f"Monitoring spike activity for {duration_ms}ms...")
    
    # Load deployment info
    deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
    if not os.path.exists(deployment_info_file):
        print("Error: No SNN deployed", file=sys.stderr)
        return 1
    
    with open(deployment_info_file, 'r') as f:
        info = json.load(f)
    
    deployment_plan = info['deployment_plan']
    config = ClusterConfig(args.config)
    
    # Collect spikes from all backplanes
    all_spikes = []
    
    for bp_name in deployment_plan['backplane_nodes'].keys():
        bp = config.get_backplane(bp_name)
        if not bp:
            continue
        
        try:
            client = Z1Client(controller_ip=bp.controller_ip)
            spikes = client.get_spike_activity(duration_ms=duration_ms)
            
            # Tag spikes with backplane
            for spike in spikes:
                spike.backplane = bp_name
            
            all_spikes.extend(spikes)
            print(f"  {bp_name}: {len(spikes)} spikes")
        except Exception as e:
            print(f"  {bp_name}: ERROR - {e}", file=sys.stderr)
    
    print(f"\nTotal spikes captured: {len(all_spikes)}")
    
    if all_spikes:
        spike_rate = (len(all_spikes) * 1000.0) / duration_ms
        print(f"Spike rate: {spike_rate:.2f} Hz")
    
    return 0


def inject_spikes(args):
    """Inject input spikes."""
    print(f"Loading spike pattern: {args.pattern}")
    
    with open(args.pattern, 'r') as f:
        pattern = json.load(f)
    
    spikes = pattern.get('spikes', [])
    
    print(f"Injecting {len(spikes)} spikes...")
    
    # Load deployment info to find which backplane has input neurons
    deployment_info_file = os.path.expanduser('~/.neurofab/last_deployment.json')
    if not os.path.exists(deployment_info_file):
        print("Error: No SNN deployed", file=sys.stderr)
        return 1
    
    with open(deployment_info_file, 'r') as f:
        info = json.load(f)
    
    deployment_plan = info['deployment_plan']
    neuron_map = {int(k): v for k, v in deployment_plan['neuron_map'].items()}
    
    # Group spikes by backplane
    spikes_by_backplane = {}
    for spike in spikes:
        neuron_id = spike['neuron_id']
        if neuron_id in neuron_map:
            bp_name, node_id, local_id = neuron_map[neuron_id]
            if bp_name not in spikes_by_backplane:
                spikes_by_backplane[bp_name] = []
            spikes_by_backplane[bp_name].append(spike)
    
    # Inject spikes to each backplane
    config = ClusterConfig(args.config)
    
    for bp_name, bp_spikes in spikes_by_backplane.items():
        bp = config.get_backplane(bp_name)
        if not bp:
            continue
        
        try:
            client = Z1Client(controller_ip=bp.controller_ip)
            count = client.inject_spikes(bp_spikes)
            print(f"  {bp_name}: {count} spikes injected ✓")
        except Exception as e:
            print(f"  {bp_name}: ERROR - {e}", file=sys.stderr)
    
    print(f"\n✓ Spikes injected")
    return 0


def main():
    parser = argparse.ArgumentParser(
        description='Manage Spiking Neural Networks on Z1 cluster',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Commands:
  deploy TOPOLOGY       Deploy SNN from topology JSON file
  status                Show SNN status and statistics
  start                 Start SNN execution
  stop                  Stop SNN execution
  monitor DURATION      Monitor spike activity (milliseconds)
  inject PATTERN        Inject input spikes from JSON file

Examples:
  nsnn deploy network.json                  # Deploy to default backplane
  nsnn deploy network.json --all            # Deploy across all backplanes
  nsnn status                               # Show deployment status
  nsnn start                                # Start execution
  nsnn monitor 5000                         # Monitor for 5 seconds
  nsnn inject input.json                    # Inject input pattern
  nsnn stop                                 # Stop execution
        """
    )
    
    parser.add_argument('command', 
                       choices=['deploy', 'status', 'start', 'stop', 'monitor', 'inject'],
                       help='Command to execute')
    parser.add_argument('argument', nargs='?',
                       help='Command argument (topology file, duration, pattern file)')
    parser.add_argument('-c', '--controller',
                       default='192.168.1.222',
                       help='Controller IP address (single backplane mode)')
    parser.add_argument('--config',
                       help='Cluster configuration file')
    parser.add_argument('--all',
                       action='store_true',
                       help='Use all configured backplanes')
    
    args = parser.parse_args()
    
    try:
        if args.command == 'deploy':
            if not args.argument:
                print("Error: topology file required", file=sys.stderr)
                return 1
            args.topology = args.argument
            return deploy_snn(args)
        
        elif args.command == 'status':
            return status_snn(args)
        
        elif args.command == 'start':
            return start_snn(args)
        
        elif args.command == 'stop':
            return stop_snn(args)
        
        elif args.command == 'monitor':
            if not args.argument:
                print("Error: duration required", file=sys.stderr)
                return 1
            args.duration = int(args.argument)
            return monitor_snn(args)
        
        elif args.command == 'inject':
            if not args.argument:
                print("Error: pattern file required", file=sys.stderr)
                return 1
            args.pattern = args.argument
            return inject_spikes(args)
    
    except Z1ClusterError as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1
    except KeyboardInterrupt:
        print("\nInterrupted", file=sys.stderr)
        return 130
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
